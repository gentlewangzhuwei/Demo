{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "- [Obtaining the IMDb movie review dataset](#Obtaining-the-IMDb-movie-review-dataset)\n",
    "- [Text-feature-extraction](#Text-feature-extraction)\n",
    "    - [Bag-of-words model](#Bag-of-words-model)\n",
    "    - [Bigrams and N-Grams](#Bigrams-and-N-Grams)\n",
    "    - [Character n-grams](#Character-n-grams)\n",
    "    - [Tfidf encoding](#Tfidf-encoding)\n",
    "- [Cleaning text data](#Cleaning-text-data)\n",
    "- [Processing documents into tokens](#Processing-documents-into-tokens)\n",
    "- [Training a logistic regression model for sentiment classification](#Training-a-logistic-regression-model-for-sentiment-classification)\n",
    "- [Working with bigger data - online algorithms and out-of-core learning](#Working-with-bigger-data---online-algorithms-and-out-of-core-learning)\n",
    "- [Model persistence](#Model-persistence)\n",
    "\n",
    "- [word2vec](#word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the IMDb movie review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据可从[这](http://ai.stanford.edu/~amaas/data/sentiment/)下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解压之后，下面代码可将数据读成 Pandas 的 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!"
     ]
    }
   ],
   "source": [
    "cat 0_9.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:57\n"
     ]
    }
   ],
   "source": [
    "# pip install pyprind\n",
    "import pyprind   # module for Python Progress Indicator\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "labels = {'pos':1, 'neg':0}  # 1 = positive and 0 = negative\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path ='data/aclImdb/%s/%s' % (s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Saving the assembled data as CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./data/movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free text with variables length is very far from the fixed length numeric representation that we need to do machine learning with scikit-learn.  \n",
    "However, there is an easy and effective way to go from text data to a numeric representation using the so-called [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model), which provides a data structure that is compatible with the machine learning aglorithms in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'is': 1,\n",
       " 'one': 2,\n",
       " 'shining': 3,\n",
       " 'sun': 4,\n",
       " 'sweet': 5,\n",
       " 'the': 6,\n",
       " 'two': 7,\n",
       " 'weather': 8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'is', 'one', 'shining', 'sun', 'sweet', 'the', 'two', 'weather']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from executing the preceding command, the vocabulary is stored in a Python dictionary, which maps the unique words that are mapped to integer indices. Next let us print the feature vectors that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [2, 3, 2, 1, 1, 1, 2, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()  # 每行对应着一个 document， 每列对应一个 word， 值是 word 对应的计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['the', 'sun', 'is', 'shining'], \n",
       "       dtype='<U7'), array(['the', 'is', 'weather', 'sweet'], \n",
       "       dtype='<U7'), array(['the', 'sun', 'is', 'shining', 'weather', 'sweet', 'and', 'one',\n",
       "        'two'], \n",
       "       dtype='<U7')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.inverse_transform(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams and N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last section, we used the so-called 1-gram (unigram) tokenization: Each token represents a single element with regard to the splittling criterion. \n",
    "\n",
    "Entirely discarding word order is not always a good idea, as composite phrases often have specific meaning, and modifiers like \"not\" can invert the meaning of words.\n",
    "\n",
    "A simple way to include some word order are n-grams, which don't only look at a single token, but at all pairs of neighborhing tokens. For example, in 2-gram (bigram) tokenization, we would group words together with an overlap of one word; in 3-gram (trigram) splits we would create an overlap two words, and so forth:\n",
    "\n",
    "- original text: \"this is how you get ants\"\n",
    "- 1-gram: \"this\", \"is\", \"how\", \"you\", \"get\", \"ants\"\n",
    "- 2-gram: \"this is\", \"is how\", \"how you\", \"you get\", \"get ants\"\n",
    "- 3-gram: \"this is how\", \"is how you\", \"how you get\", \"you get ants\"\n",
    "\n",
    "Which \"n\" we choose for \"n-gram\" tokenization to obtain the optimal performance in our predictive model depends on the learning algorithm, dataset, and task. Or in other words, we have consider \"n\" in \"n-grams\" as a tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer class in scikit-learn allows us to use different n-gram models via its ngram_range parameter. While a 1-gram representation is used by default, we could switch to a 2-gram representation by initializing a new CountVectorizer instance with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_vectorizer.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and one': 0,\n",
       " 'is shining': 1,\n",
       " 'is sweet': 2,\n",
       " 'is two': 3,\n",
       " 'one and': 4,\n",
       " 'one is': 5,\n",
       " 'shining the': 6,\n",
       " 'sun is': 7,\n",
       " 'sweet and': 8,\n",
       " 'the sun': 9,\n",
       " 'the weather': 10,\n",
       " 'weather is': 11}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is also helpful not only to look at words, but to consider single characters instead.   \n",
    "That is particularly useful if we have very noisy data and want to identify the language, or if we want to predict something about a single word.\n",
    "We can simply look at characters instead of words by setting ``analyzer=\"char\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ['Some say the world will end in fire,', 'Some say in ice.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectorizer = CountVectorizer(analyzer=\"char\")\n",
    "char_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ',', '.', 'a', 'c', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(char_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called term frequency-inverse document frequency (tf-idf) that can be used to downweight those frequently occurring words in the feature vectors. The tf-idf can be de ned as the product of the term frequency and the inverse document frequency:\n",
    "\n",
    "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
    "\n",
    "Here the tf(t, d) is the term frequency that we introduced in the previous section,\n",
    "and the inverse document frequency *idf(t, d)* can be calculated as:\n",
    "\n",
    "$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n",
    "\n",
    "where $n_d$ is the total number of documents, and *df(d, t)* is the number of documents *d* that contain the term *t*. Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training samples; the log is used to ensure that low document frequencies are not given too much weight.\n",
    "\n",
    "Scikit-learn implements yet another transformer, the `TfidfTransformer`, that takes the raw term frequencies from `CountVectorizer` as input and transforms them into tf-idfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.    0.56  0.56  0.    0.43  0.    0.  ]\n",
      " [ 0.    0.43  0.    0.    0.    0.56  0.43  0.    0.56]\n",
      " [ 0.5   0.45  0.5   0.19  0.19  0.19  0.3   0.25  0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous subsection, the word \"is\" (column 2) had the largest term frequency in the 3rd document, being the most frequently occurring word. However, after transforming the same feature vector into tf-idfs, we see that the word \"is\" is now associated with a relatively small tf-idf (0.45) in document 3 since it is also contained in documents 1 and 2 and thus is unlikely to contain any useful, discriminatory information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we'd manually calculated the tf-idfs of the individual terms in our feature vectors, we'd have noticed that the `TfidfTransformer` calculates the tf-idfs slightly differently compared to the standard textbook equations that we see earlier. The equations for the idf and tf-idf that were implemented in scikit-learn are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
    "\n",
    "The tf-idf equation that was implemented in scikit-learn is as follows:\n",
    "\n",
    "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
    "\n",
    "While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the `TfidfTransformer` normalizes the tf-idfs directly.\n",
    "\n",
    "By default (`norm='l2'`), scikit-learn's TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector *v* by its L2-norm:\n",
    "\n",
    "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
    "\n",
    "To make sure that we understand how TfidfTransformer works, let us walk\n",
    "through an example and calculate the tf-idf of the word is in the 3rd document.\n",
    "\n",
    "The word is has a term frequency of 3 (tf = 3) in document 3, and the document frequency of this term is 3 since the term is occurs in all three documents (df = 3). Thus, we can calculate the idf as follows:\n",
    "\n",
    "$$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3} = 0$$\n",
    "\n",
    "Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency:\n",
    "\n",
    "$$\\text{tf-idf}(\"is\",d3)= 3 \\times (0+1) = 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf of term \"is\" = 3.00\n"
     ]
    }
   ],
   "source": [
    "tf_is = 3\n",
    "n_docs = 3\n",
    "idf_is = np.log((n_docs+1) / (3+1))\n",
    "tfidf_is = tf_is * (idf_is + 1)\n",
    "print('tf-idf of term \"is\" = %.2f' % tfidf_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeated these calculations for all terms in the 3rd document, we'd obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]. However, we notice that the values in this feature vector are different from the values that we obtained from the TfidfTransformer that we used previously. The step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$\n",
    "\n",
    "$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n",
    "\n",
    "$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.39,  3.  ,  3.39,  1.29,  1.29,  1.29,  2.  ,  1.69,  1.29])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
    "raw_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5 ,  0.45,  0.5 ,  0.19,  0.19,  0.19,  0.3 ,  0.25,  0.19])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results match the results returned by scikit-learn's `TfidfTransformer` (below). Since we now understand how tf-idfs are calculated, let us proceed to the next sections and apply those concepts to the movie review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]  # last 50 characters from the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text contains html markup tags, we need clean them.\n",
    "we will now remove all punctuation marks but only keep emoticon characters such as \":)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re #  Python 中处理正则表达式的模块\n",
    "def preprocessor(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text)  # 去除 HTML 标签\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub(r'\\W+', ' ', text.lower()) + \\\n",
    "           ' '.join(emoticons).replace('-', '')   # 将表情符号拼接在正文后面\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing documents into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the text corpora into individual elements.\n",
    "\n",
    "tokenize 是把长文本切成一系列单词，最简单的方式就是以 whitespace 切分.\n",
    "\n",
    "word stemming 是将词转为最原始的形式, root form, (例如 running -> run), 一种算法是 Porter stemmer algorithm\n",
    "\n",
    "以下需要使用 nltk, 需要先安装: `pip install nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()  # 默认以空格切分\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop-words (停词) 是 最常见的一些单词, 它们的实际意义并不是很大, 大多是起辅助作用的, 但是它们的频次非常高, 所以需要去除, 例如 is, and, has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiaokai/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')  # 下载停词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a logistic regression model for sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip HTML and punctuation to speed up the GridSearch later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  25,000 documents for training and 25,000 documents for testing, 需要大约40分钟\n",
    "# 所以先使用5000 documents\n",
    "X_train = df.loc[:5000, 'review'].values\n",
    "y_train = df.loc[:5000, 'sentiment'].values\n",
    "X_test = df.loc[5000:, 'review'].values\n",
    "y_test = df.loc[5000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer 等同于 CountVectorizer + TfidfTransformer\n",
    "tfidf = TfidfVectorizer(strip_accents=None, \n",
    "                        lowercase=False, \n",
    "                        preprocessor=None)\n",
    "\n",
    "# grig search\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "             {'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "             ]\n",
    "\n",
    "# 先转化为 tfidf matrix\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, \n",
    "                           scoring='accuracy',\n",
    "                           cv=5, verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 13.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'clf__penalty': ['l1', 'l2'], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 't...ect__tokenizer': [<function tokenizer at 0x11ad34158>, <function tokenizer_porter at 0x11ad340d0>]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据量减少为5000 后需要时间大约为8分钟\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__penalty': 'l2', 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'vect__ngram_range': (1, 1), 'clf__C': 10.0, 'vect__tokenizer': <function tokenizer at 0x11ad34158>} \n",
      "CV Accuracy: 0.862\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.872\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Working with bigger data - online algorithms and out-of-core learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-Core learning is the task of training a machine learning model on a dataset that does not fit into memory or RAM. This requires the following conditions:\n",
    "    \n",
    "- a **feature extraction** layer with **fixed output dimensionality**\n",
    "- knowing the list of all classes in advance (in this case we only have positive and negative tweets)\n",
    "- a machine learning **algorithm that supports incremental learning** (the `partial_fit` method in scikit-learn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):  # 生成器\n",
    "    # generator funciton, reads in and returns one document at a time\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To verify that our stream_docs function works correctly\n",
    "gen = stream_docs(path='./data/movie_data.csv')\n",
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    '''\n",
    "    take a document stream from the stream_docs function and \n",
    "    return a particular number of documents\n",
    "    '''\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer  # makes use of the Hashing trick\n",
    "from sklearn.linear_model import SGDClassifier  # train a logistic regression model using small minibatches of documents\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path='./data/movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:39\n"
     ]
    }
   ],
   "source": [
    "# iterated over 45 minibatches of documents \n",
    "# where each minibatch consists of 1,000 documents each\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "#use the last 5,000 documents to evaluate the performance\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然准确率略低于前面，但训练速度快了很多，而且使用的内存更少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the last 5,000 documents to update our model\n",
    "# 可以使用 partial_fit 继续训练\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model persistence\n",
    "\n",
    "训练模型是 expensive 并且耗费时间的, 我们不希望在应用中每次都要重新训练模型, 所以我们需要保存模型, 并且能进行新的预测以及更新。\n",
    "可以用到 pickle 模块来储存模型, 将 python object 储存为byte code, 可以读取也可以写入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the logistic regression model as shown above, we can save the classifier along with the stop words, Porter Stemmer, and `HashingVectorizer` as serialized objects to our local disk so that we can use the fitted classifier in our web application later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "#  created a movieclassifier directory \n",
    "# created a pkl_objects subdirectory to save the serialized Python objects to our local drive\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "# 写入\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=2)   \n",
    "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save the `HashingVectorizer` as in a separate file so that we can import it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing movieclassifier/vectorizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile movieclassifier/vectorizer.py\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the preceeding code cells, we can now restart the IPython notebook kernel to check if the objects were serialized correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, change the current Python directory to `movieclassifer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('movieclassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect\n",
    "\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 82.52%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = ['I love this movie']\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[clf.predict(X)[0]], np.max(clf.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec 是 [Mikolov et al.](http://arxiv.org/pdf/1301.3781.pdf) 提出一种训练词向量的方法。它有 Continuous Bag-of-Words model (CBOW) and the Skip-Gram model 两种变式，前者是用一个词序列窗口中的其他词来预测中心词，后者则是用中心词来预测其他词。在实际使用 word2vec 时，一般使用 Skip-Gram model 结合 [Negative Sampling](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) 进行训练。\n",
    "\n",
    "![CBOW & skip-gram](figures/word2vec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gensim 库中包含了 word2vec 模块\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练词向量， gensim 训练词向量时可以喂入一个 iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MySentence:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for line in self.data:\n",
    "            yield line.lower().split()\n",
    "        \n",
    "train_corpus = MySentence(df['review'])\n",
    "\n",
    "# 或者 train_corpus = [s.split() for s in df['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(train_corpus, \n",
    "                 size=200, # 词向量的维度\n",
    "                 iter=20, # 数据在训练中用到的次数, 即 epoch 数\n",
    "                 workers=4)  # 调用的进程数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单查看词向量的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7370797991752625),\n",
       " ('bad', 0.7096108198165894),\n",
       " ('great', 0.6964930295944214),\n",
       " ('nice', 0.6179347634315491),\n",
       " ('passable', 0.6055103540420532),\n",
       " ('cool', 0.5933259129524231),\n",
       " ('fine', 0.5922046899795532),\n",
       " ('funny', 0.5853421688079834),\n",
       " ('lousy', 0.5655009746551514),\n",
       " ('weak', 0.5625821352005005)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得词对应的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.16470599,  2.21028686, -2.42560244, -0.00975356,  0.38119075,\n",
       "        0.28335837,  0.42377841,  1.30055726, -0.86984408, -0.0956203 ,\n",
       "       -0.51766461, -1.41315424,  0.18915185, -2.9125576 , -2.36487603,\n",
       "        3.38622642, -1.72619522, -0.6556592 ,  0.21282369,  1.09593034,\n",
       "       -1.89328766, -1.39257145, -3.33210802, -1.55650818,  0.67174482,\n",
       "        1.74836183, -1.9717145 ,  3.95806265,  1.53746414,  1.82085156,\n",
       "        1.02304912, -1.3881073 ,  0.32539955, -0.84898555,  0.55216944,\n",
       "       -1.15237427, -0.86884212,  0.0770219 , -2.18501925,  0.58333641,\n",
       "       -1.10986626,  0.84115869,  3.0748837 , -1.02027082,  0.76942962,\n",
       "        0.19782287, -0.68174469, -0.98301965, -1.85113227, -1.31748915,\n",
       "       -2.22454476, -0.40986761, -1.53826535, -1.63622868,  0.99653172,\n",
       "        0.28070587, -0.92813587, -0.72385532, -1.55703568,  0.36305025,\n",
       "       -0.32554394, -2.66813493, -1.6127274 ,  0.70773345,  0.10997313,\n",
       "       -1.41711199,  0.05896465,  0.62872934, -2.60525918, -0.54656357,\n",
       "       -0.3445617 ,  0.26571736, -1.90026855, -1.43783331,  0.70140201,\n",
       "       -1.2951417 , -0.00762774, -3.05872321, -0.74876642, -0.32890773,\n",
       "        1.17948925, -0.93165678,  0.71599245,  0.42433202,  3.10504317,\n",
       "       -0.5427857 , -0.15808412, -1.82514787, -0.35076511,  0.82464337,\n",
       "       -1.34721267,  0.73901194, -0.43170625, -0.7415626 ,  0.14467889,\n",
       "        1.56619215,  2.85391712, -0.57105207,  0.50426775, -1.84612775,\n",
       "        0.85882884, -0.3593798 , -4.95663691,  1.60039163,  1.09668696,\n",
       "        3.45201159, -2.31795955, -2.08626485,  2.38695407,  0.85859919,\n",
       "       -0.51280987, -3.20326591, -0.37576312, -1.37056541,  1.3122443 ,\n",
       "        1.6884706 ,  0.06221735, -3.43917394, -0.07093932,  0.31405959,\n",
       "        1.14408076, -3.09645104,  0.79853904, -1.39219332, -1.25883484,\n",
       "       -1.1431123 , -2.72376561,  1.0773927 ,  0.45701203,  0.42897376,\n",
       "        2.48681235,  2.33315301,  0.70380259, -0.4545216 , -0.65549785,\n",
       "       -0.49024123,  1.594033  , -0.49076727, -2.30712199,  0.05816827,\n",
       "        0.52932864,  2.74852037,  0.60267115,  0.56647575, -1.66480327,\n",
       "        0.73315042, -1.03744304, -1.34164691, -0.70087159,  0.65839398,\n",
       "       -1.01345015,  1.20440805, -1.64834189,  1.89878547,  0.10761415,\n",
       "        1.06873488, -1.86278129, -2.42350578,  1.70184171, -2.3078649 ,\n",
       "       -0.35207719,  0.14962029,  0.87053704,  0.70458615,  1.31776845,\n",
       "       -1.55921471,  0.31351587,  1.36400843,  1.06291854, -2.04706645,\n",
       "       -0.3120206 , -0.08525608, -1.5200597 , -3.25832319, -1.54889965,\n",
       "        0.84222311, -1.90415549, -2.76563501, -1.11342561,  0.56705254,\n",
       "       -1.13262045,  1.35900307,  2.66902637, -1.08058667, -0.20109835,\n",
       "       -0.18454814,  2.62732744,  1.25543046, -1.21116841,  1.14051116,\n",
       "        0.66510123, -0.12158651,  3.42188811,  1.54110575, -2.0953362 ,\n",
       "       -0.43784541,  0.39110315,  0.48031813, -0.23427317,  1.83466482], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存储/读取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('data/imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('data/imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用训练好的词向量来进行情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 用 word vec 的均值作为 doc vec\n",
    "def get_doc_vec(sentence, model):\n",
    "    scores = [model[word] for word in sentence.split() \n",
    "              if word in model]  # 如果词频小于 min_count, word2vec 不会把这个词放入 vocab 里\n",
    "    \n",
    "    return np.mean(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_word2vec_train = np.array([get_doc_vec(sentence, model) for sentence in X_train])\n",
    "X_word2vec_test = np.array([get_doc_vec(sentence, model) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   15.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=0)\n",
    "\n",
    "param_grid = [{'penalty': ['l1', 'l2'],\n",
    "               'C': [1.0, 10.0, 100.0]}]\n",
    "\n",
    "gs = GridSearchCV(lr, param_grid, \n",
    "                  scoring='accuracy',\n",
    "                  cv=5, verbose=1,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs.fit(X_word2vec_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86322735452909416"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.869\n"
     ]
    }
   ],
   "source": [
    "clf = gs.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_word2vec_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文新闻分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/SogouC.reduced/Reduced/C000008',\n",
       " 'data/SogouC.reduced/Reduced/C000010',\n",
       " 'data/SogouC.reduced/Reduced/C000013',\n",
       " 'data/SogouC.reduced/Reduced/C000014',\n",
       " 'data/SogouC.reduced/Reduced/C000016',\n",
       " 'data/SogouC.reduced/Reduced/C000020',\n",
       " 'data/SogouC.reduced/Reduced/C000022',\n",
       " 'data/SogouC.reduced/Reduced/C000023',\n",
       " 'data/SogouC.reduced/Reduced/C000024']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootdir = 'data/SogouC.reduced/Reduced'\n",
    "dirs = os.listdir(rootdir)\n",
    "dirs = [path.join(rootdir,f) for f in dirs if f.startswith('C')]\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?codecs.open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "def load_file(input_file):\n",
    "    input_data = codecs.open(input_file,mode= 'r',encoding= 'gbk',errors='ignore')\n",
    "    input_text = input_data.read()\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　本报记者陈雪频实习记者唐翔发自上海\r\n",
      "　　一家刚刚成立两年的网络支付公司，它的目标是成为市值100亿美元的上市公司。\r\n",
      "　　这家公司叫做快钱，说这句话的是快钱的CEO关国光。他之前曾任网易的高级副总裁，负责过网易的上市工作。对于为什么选择第三方支付作为创业方向，他曾经对媒体这样说：“我能看到这个胡同对面是什么，别人只能看到这个胡同。”自信与狂妄只有一步之遥——这几乎是所有创业者的共同特征，是自信还是狂妄也许需要留待时间来考证。\r\n",
      "　　对于市值100亿美元的上市公司，他是这样算这笔账的，“百度上市时广告客户数量只有4万,而且它所做的只是把客户吸引过来，就可以支撑起现有的庞大市值；而我们几年后的客户数量是几千万，而且这些客户都是能直接带来利润的，说市值100亿美元一点都不夸张。”\r\n",
      "　　这家公司2005年年底注册用户达到400万，计划今年注册用户突破1000万，号称是国内最大的第三方网络支付平台。“在美国跟支付相关的收入已经超过了所有商业银行本身利差收入的总和，我所查到的数据是3000亿美元，其中超过70％是个人消费者带来的收入。”关国光喜欢借用美国支付产业的现状与中国的情况进行比较。虽然美国和中国差异显著，但他坚信中国的第三方支付市场前景非常广阔。\r\n",
      "　　便利和安全挑战网络支付\r\n",
      "　　“你只需要一个手机号码或者一个邮件地址就可以网络支付。”在快钱的户外广告中这样写道，这和传统的需要银行账户才能进行网络支付的习惯形成了鲜明的对比。\r\n",
      "　　然而这种支付模式和传统的网络支付并无本质的区别，因为每一个手机号码和邮件地址背后都会对应着一个账户——这个账户可以是信用卡账户、借记卡账户，也包括邮局汇款、手机代收、电话代收、预付费卡和点卡等多种形式。\r\n",
      "　　“快钱的功能其实就相当于融合了很多交易工具的VISA卡，所以又被称为网络VISA。”关国光说，“从本质上讲，我们和VISA等采用的底层技术是没有差别的，我们和它的区别在于VISA卡面对的交易工具比较单一，而快钱面对的是多种分散的交易工具。”\r\n",
      "　　因为“信用缺位”，网络支付一直是困扰中国电子商务发展的瓶颈之一。网络支付平台相当于“信用缺位”条件下的“补位产物”，它把众多的银行卡整合到一个页面端口，以支付公司作为信用中介，在买家确认收到商品前，代替买卖双方暂时保管货款。\r\n",
      "　　目前最知名的网络支付平台包括阿里巴巴的支付宝和eBay的Paypal（贝宝）。关国光表示，快钱最大的特点是第三方的支付平台，主要客户为那些中小公司。这些网络支付平台的主要业务是针对母公司的，不太可能被其母公司同行使用。\r\n",
      "　　“而用户可以选择使用从银行卡、邮政汇款到点卡、预付费卡的各种支付方式，快钱平台对人口和支付工具的覆盖都非常广泛，这是我们创新的地方。”关国光告诉《第一财经日报》。\r\n",
      "　　交易的安全性是网上支付平台最大的问题。关国光说：“快钱采用了各种机制来保证用户资金的安全性，例如回款机制等，可以在用户付款过程由于各种意外因素中止或未完成时，系统将用户账户自动回复到交易开始时的原始状态。”除此之外，快钱也建立了一系列监控机制，有问题的交易系统会被强制暂停两天以供审查。内部财务方面则遵守相互监督的原则，不会让任何一个人参与主导交易全程。在交易的识别方面，快钱有一套交易过滤引擎，可以识别出较明显的问题交易行为。\r\n",
      "　　增值服务：网络支付的撒手锏\r\n",
      "　　在关国光看来，快钱平台可以提供详细的用户行为记录，进而方便商户掌握用户的喜好和需要。这种增值服务对于支付平台至关重要。\r\n",
      "　　在市场推广方面，快钱公司市场推广的目标就是让用户知道快钱这个品牌，然后将精力集中在核心应用上。“快钱的策略就是同主流应用相捆绑，致力同大厂商合作，因为目前大型门户所掌握的用户资源是相当多的。”\r\n",
      "　　“我们不必去强迫用户使用快钱，因为网络支付只是交易过程中的一个附加品，单推一个支付平台是没有效果的。”关国光表示，“快钱的策略就是要找准交易，捆绑和依托在上面，用户在进行电子交易时，自然而然就会用到我们的支付平台。”\r\n",
      "　　除此之外，快钱的策略是先稳定一批活跃用户后，再将目光放到普通用户上。关国光把快钱的营销模式称为非线型营销，“一个注册用户在支付交易中往往会带入一个未注册用户，用户之间的互动会形成网络效应。”\r\n",
      "　　从创业开始，快钱的商业模式就没有变化过。“快钱的后端绝不做应用。”关国光信誓旦旦地表示。他认为一旦快钱做了应用，将面临更多的竞争对手，同时也违背了自己独立的第三方支付平台的市场定位。\r\n",
      "　　快钱未来的梦想是：首先，将所有的支付工具整合为一个接口，随着流量的增加，给商户带来的价值是一样的，但成本更低。其次，快钱拥有用户的详细数据资料，可以实现数据库营销，为客户提供增值服务，例如支持客户的促销、推广等，或者为客户创造商业机会。\r\n",
      "　　2005年8月，快钱公司获得美国DCM和半岛基金的首批风险投资，第二轮目前还在评估之中。关国光表示，对快钱来说，融资是一件水到渠成的事情，是最后考虑的问题。快钱挑选风投的条件只有两个，一是必须在中国有过投资；二是在中国的投资必须取得过成功。\r\n",
      "　　“电子支付行业是一个入行容易、生存难的行业，”关国光说，“任何企业要在这个领域取得成功，就必须脚踏实地地认真去做，绝不能抱着投机的心理。”\n"
     ]
    }
   ],
   "source": [
    "print(load_file('data/SogouC.reduced/Reduced/C000008/10.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/SogouC.reduced/Reduced/C000008',\n",
       " 'data/SogouC.reduced/Reduced/C000010',\n",
       " 'data/SogouC.reduced/Reduced/C000013',\n",
       " 'data/SogouC.reduced/Reduced/C000014',\n",
       " 'data/SogouC.reduced/Reduced/C000016',\n",
       " 'data/SogouC.reduced/Reduced/C000020',\n",
       " 'data/SogouC.reduced/Reduced/C000022',\n",
       " 'data/SogouC.reduced/Reduced/C000023',\n",
       " 'data/SogouC.reduced/Reduced/C000024']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_t = {}\n",
    "for i, d in enumerate(dirs):\n",
    "    files = os.listdir(d)\n",
    "    files = [path.join(d, x) for x in files if x.endswith('txt') and not x.startswith('.')]\n",
    "    text_t[i] = [load_file(f) for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　本报记者陈雪频实习记者唐翔发自上海\r\n",
      "　　一家刚刚成立两年的网络支付公司，它的目标是成为市值100亿美元的上市公司。\r\n",
      "　　这家公司叫做快钱，说这句话的是快钱的CEO关国光。他之前曾任网易的高级副\n"
     ]
    }
   ],
   "source": [
    "print(text_t[0][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flen = [len(t) for t in text_t.values()]\n",
    "labels = np.repeat(list(text_t.keys()),flen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatter nested list\n",
    "import itertools\n",
    "merged = list(itertools.chain.from_iterable(text_t.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>本报记者陈雪频实习记者唐翔发自上海\\r\\n　　一家刚刚成立两年的网络支付公司，它的目标是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>证券通：百联股份未来5年有能力保持高速增长\\r\\n\\r\\n    深度报告 权威内参...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                txt\n",
       "0      0  　　本报记者陈雪频实习记者唐翔发自上海\\r\\n　　一家刚刚成立两年的网络支付公司，它的目标是...\n",
       "1      0      证券通：百联股份未来5年有能力保持高速增长\\r\\n\\r\\n    深度报告 权威内参...\n",
       "2      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....\n",
       "3      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....\n",
       "4      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www...."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'label': labels, 'txt': merged})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/7t/2bxpnffn2r9fdg94r9fk6t4h0000gn/T/jieba.cache\n",
      "Loading model cost 1.690 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# cut word\n",
    "import jieba\n",
    "jieba.enable_parallel(4)\n",
    "def cutword_1(x):\n",
    "    words = jieba.cut(x)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['seg_word'] = df.txt.map(cutword_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>txt</th>\n",
       "      <th>seg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>本报记者陈雪频实习记者唐翔发自上海\\r\\n　　一家刚刚成立两年的网络支付公司，它的目标是...</td>\n",
       "      <td>本报记者 陈雪频 实习 记者 唐翔 发自 上海 \\r\\n 　 　 一家 刚刚 成立 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>证券通：百联股份未来5年有能力保持高速增长\\r\\n\\r\\n    深度报告 权威内参...</td>\n",
       "      <td>证券 通 ： 百联 股份 未来 5 年 有 能力 保持高速 增长 \\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "      <td>5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "      <td>5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "      <td>5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                txt  \\\n",
       "0      0  　　本报记者陈雪频实习记者唐翔发自上海\\r\\n　　一家刚刚成立两年的网络支付公司，它的目标是...   \n",
       "1      0      证券通：百联股份未来5年有能力保持高速增长\\r\\n\\r\\n    深度报告 权威内参...   \n",
       "2      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....   \n",
       "3      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....   \n",
       "4      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....   \n",
       "\n",
       "                                            seg_word  \n",
       "0  　 　 本报记者 陈雪频 实习 记者 唐翔 发自 上海 \\r\\n 　 　 一家 刚刚 成立 ...  \n",
       "1          证券 通 ： 百联 股份 未来 5 年 有 能力 保持高速 增长 \\r\\n ...  \n",
       "2          5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...  \n",
       "3          5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...  \n",
       "4          5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pickle import dump,load\n",
    "dump(df, open('data/tmdf.pickle', 'wb'))\n",
    "# df = load(open('df.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17910, 10000)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), min_df = 2, max_features = 10000)\n",
    "xvec = vect.fit_transform(df.seg_word)\n",
    "xvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "train_X, test_X, train_y, test_y = train_test_split(xvec, y , train_size=0.7, random_state=1)\n",
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.88      0.89       577\n",
      "          1       0.89      0.81      0.85       603\n",
      "          2       0.88      0.82      0.85       619\n",
      "          3       0.98      0.97      0.98       584\n",
      "          4       0.86      0.88      0.87       570\n",
      "          5       0.88      0.79      0.83       600\n",
      "          6       0.77      0.90      0.83       600\n",
      "          7       0.76      0.83      0.80       615\n",
      "          8       0.93      0.93      0.93       605\n",
      "\n",
      "avg / total       0.87      0.87      0.87      5373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "pre = clf.predict(test_X)\n",
    "print(metrics.classification_report(test_y, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 100\n",
    "min_word_count = 10\n",
    "num_workers = 4\n",
    "context = 5\n",
    "epoch = 20\n",
    "sample = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec\n",
    "txt = df.seg_word.values\n",
    "txtlist = []\n",
    "for sent in txt:\n",
    "    temp = [w for w in sent.split()]\n",
    "    txtlist.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(txtlist, workers = num_workers,\n",
    "                          sample = sample,\n",
    "                          size = num_features,\n",
    "                          min_count=min_word_count,\n",
    "                          window = context,\n",
    "                          iter = epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57669, 100)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络 0.7809788584709167\n",
      "门户网站 0.7471935749053955\n",
      "网络广告 0.7400577068328857\n",
      "无线 0.7217098474502563\n",
      "搜索引擎 0.716282308101654\n",
      "在线 0.7109969854354858\n",
      "网民 0.7070432901382446\n",
      "服务商 0.7070391178131104\n",
      "B2B 0.6956398487091064\n",
      "艾瑞 0.6875394582748413\n"
     ]
    }
   ],
   "source": [
    "for w in model.most_similar('互联网'):\n",
    "    print(w[0], w[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save('sogo_wv')\n",
    "#model = word2vec.Word2Vec.load('sogo_wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将词向量平均化为文档向量 \n",
    "def sentvec_1(sent,m=num_features,model=model): \n",
    "    res = np.zeros(m) \n",
    "    words = sent.split() \n",
    "    num = 0  \n",
    "    for w in words: \n",
    "        if w in model.index2word: \n",
    "            res += model[w] \n",
    "            num += 1.0 \n",
    "    if num == 0: return np.zeros(m) \n",
    "    else: return res/num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 02:44:56\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(len(df.seg_word.values))\n",
    "n = df.shape[0] \n",
    "sent_matrix = np.zeros([n,num_features],float) \n",
    "for i ,sent in enumerate(df.seg_word.values): \n",
    "    sent_matrix[i,:] = sentvec_1(sent) \n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17910, 100)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_matrix.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(sent_matrix, y , train_size=0.7, random_state=1)\n",
    "clf = GradientBoostingClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.85      0.87       577\n",
      "          1       0.83      0.80      0.81       603\n",
      "          2       0.85      0.87      0.86       619\n",
      "          3       0.97      0.97      0.97       584\n",
      "          4       0.84      0.88      0.86       570\n",
      "          5       0.83      0.80      0.81       600\n",
      "          6       0.83      0.87      0.85       600\n",
      "          7       0.76      0.79      0.78       615\n",
      "          8       0.94      0.92      0.93       605\n",
      "\n",
      "avg / total       0.86      0.86      0.86      5373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(train_X, train_y)\n",
    "from sklearn import metrics\n",
    "pre = clf.predict(test_X)\n",
    "print(metrics.classification_report(test_y, pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 练习：你来改善中文文本分类的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf3]",
   "language": "python",
   "name": "conda-env-tf3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
