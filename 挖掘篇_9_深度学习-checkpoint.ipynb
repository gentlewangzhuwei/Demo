{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction to Deep Learning with TensorFlow](#Introduction-to-Deep-Learning-with-TensorFlow)\n",
    "    - [What is Deep Learning?](#What-is-Deep-Learning?)\n",
    "    - [What is TensorFlow](#What-is-TensorFlow)\n",
    "    - [What is a Data Flow Graph](#What-is-a-Data-Flow-Graph)\n",
    "- [TensorFlow examples](#TensorFlow-examples)\n",
    "    - [Introduction](#Introduction)\n",
    "        - [Hello world](#Hello-world)\n",
    "        - [Basic Operations](#Basic-Operations)\n",
    "    - [Basic Models](#Basic-Models)\n",
    "        - [Nearest Neighbor](#Nearest-Neighbor)\n",
    "        - [Linear Regression](#Linear-Regression)\n",
    "        - [Logistic Regression](#Logistic-Regression)\n",
    "    - [Neural Networks](#Neural-Networks)\n",
    "        - [Multilayer Perceptron](#Multilayer-Perceptron)\n",
    "        - [Convolutional Neural Network](#Convolutional-Neural-Network)\n",
    "        - [Recurrent Neural Network (LSTM)](#Recurrent-Neural-Network-LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Deep learning is a particular kind of machine learning that achieves great power and ﬂexibility by learning to represent the world as a nested hierarchy of concepts, with each concept deﬁned in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.</i>\n",
    "\n",
    "<div align=\"right\">\n",
    "  I. Goodfellow, Y. Bengio, and A. Courville, \"Deep Learning.\" Book in preparation for MIT Press, 2016. <br>\n",
    "  http://www.deeplearningbook.org/\n",
    "</div>\n",
    "\n",
    "#### Representation Learning\n",
    "\n",
    "> <i>Use machine learning to discover not only the mapping from representation to output but also the representation itself. </i>\n",
    "\n",
    "<div align=\"right\">\n",
    "  I. Goodfellow, Y. Bengio, and A. Courville, \"Deep Learning.\" Book in preparation for MIT Press, 2016. <br>\n",
    "  http://www.deeplearningbook.org/\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figures/tf-logo.png)\n",
    "\n",
    "> <i>TensorFlow™ is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ A TensorFlow computation is described by a directed **graph**, which is composed of a set on **nodes**\n",
    "+ Each node represents the instantiation of an **operation**\n",
    "+ An **operation** represents an abstract computation (e.g., \"matrix multiply\", or \"add\")\n",
    "+ Clients programs interact with the TensorFlow system by creating a **Session**\n",
    "+ Computations represented as a dataflow graph where **tensors** flow along the graph edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Data Flow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i> Data flow graphs describe mathematical computation with a directed graph of nodes & edges. Nodes typically implement mathematical operations, but can also represent endpoints to feed in data, push out results, or read/write persistent variables. Edges describe the input/output relationships between nodes. These data edges carry dynamically-sized multidimensional data arrays, or tensors. The flow of tensors through the graph is where TensorFlow gets its name. Nodes are assigned to computational devices and execute asynchronously and in parallel once all the tensors on their incoming edges becomes available.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figures/tf_in_one_slide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [Session](https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html#launching-the-graph-in-a-session): TensorFlow 是在 session 中运行 computation graph\n",
    "+ [Fetches](https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html#fetches): 在 session 中执行 run()， 可以 fetch 得到 operation 的结果 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# Simple hello world using TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Create a Constant op\n",
    "# The op is added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "\n",
    "# Start tf session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run graph\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=2, b=3\n",
      "Addition with constants: 5\n",
      "Multiplication with constants: 6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Basic constant operations\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "\n",
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    print(\"a=2, b=3\")\n",
    "    print(\"Addition with constants: %i\" % sess.run(a+b))\n",
    "    print(\"Multiplication with constants: %i\" % sess.run(a*b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [Feed](https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html#feeds): 在 sess.run() 中传入 feed_dict 参数可以向对应的节点喂入数据\n",
    "+ placeholder 作为一个占位符，是数据输入的端点，必须要在 run() 中喂入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition with variables: 5\n",
      "Multiplication with variables: 6\n"
     ]
    }
   ],
   "source": [
    "# Basic Operations with variable as graph input\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Variable op. (define as input when running session)\n",
    "# tf Graph input\n",
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)\n",
    "# placeholder 是信息输入的端点\n",
    "# 在 sess 中通过 feed_dict 参数来喂入数据\n",
    "\n",
    "# Define some operations\n",
    "add = tf.add(a, b)\n",
    "mul = tf.mul(a, b)\n",
    "# 这里定义的操作是象征性的，sess.run 之后才会真正在内存中跑起来\n",
    "\n",
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run every operation with variable input\n",
    "    print(\"Addition with variables: %i\" % sess.run(add, feed_dict={a: 2, b: 3}))\n",
    "    print(\"Multiplication with variables: %i\" % sess.run(mul, feed_dict={a: 2, b: 3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# More in details:\n",
    "# Matrix Multiplication from TensorFlow official tutorial\n",
    "\n",
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "# To run the matmul op we call the session 'run()' method, passing 'product'\n",
    "# which represents the output of the matmul op.  This indicates to the call\n",
    "# that we want to get the output of the matmul op back.\n",
    "#\n",
    "# All inputs needed by the op are run automatically by the session.  They\n",
    "# typically are run in parallel.\n",
    "#\n",
    "# The call 'run(product)' thus causes the execution of threes ops in the\n",
    "# graph: the two constants and matmul.\n",
    "#\n",
    "# The output of the op is returned in 'result' as a numpy `ndarray` object.\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Variable](https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html#variables): \n",
    "+ variable 是在计算图中可训练的量\n",
    "+ 在 session 中必须先要初始化\n",
    "+ name 参数可定义 variable 在 graph 中的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct a linear model\n",
    "pred = tf.add(tf.mul(X, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "# Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer() # 定义初始化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 cost= 0.086615965 W= 0.195037 b= 1.19395\n",
      "Epoch: 0200 cost= 0.084530868 W= 0.201333 b= 1.14865\n",
      "Epoch: 0300 cost= 0.082898311 W= 0.206902 b= 1.10858\n",
      "Epoch: 0400 cost= 0.081619963 W= 0.211829 b= 1.07314\n",
      "Epoch: 0500 cost= 0.080618836 W= 0.216188 b= 1.04179\n",
      "Epoch: 0600 cost= 0.079834759 W= 0.220043 b= 1.01405\n",
      "Epoch: 0700 cost= 0.079220690 W= 0.223454 b= 0.989514\n",
      "Epoch: 0800 cost= 0.078739621 W= 0.226471 b= 0.96781\n",
      "Epoch: 0900 cost= 0.078362741 W= 0.22914 b= 0.948611\n",
      "Epoch: 1000 cost= 0.078067482 W= 0.2315 b= 0.93163\n",
      "Optimization Finished!\n",
      "Training cost= 0.0780675 W= 0.2315 b= 0.93163 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQvr/18lUjn+DF7MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ef92668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)  # variable 在 session 中必须先初始化\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})  # 每 run 一次 optimizer， 进行一次梯度下降\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 5\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))  # tf.reduce_mean 类似 np.mean()\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 cost= 0.465448593\n",
      "Epoch: 0010 cost= 0.392335341\n",
      "Epoch: 0015 cost= 0.362719573\n",
      "Epoch: 0020 cost= 0.345370396\n",
      "Epoch: 0025 cost= 0.333636161\n",
      "Optimization Finished!\n",
      "Accuracy: 0.888333\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print( \"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 5\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 175.416232144\n",
      "Epoch: 0006 cost= 9.738634324\n",
      "Epoch: 0011 cost= 2.241495019\n",
      "Optimization Finished!\n",
      "Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25600, Minibatch Loss= 748.087402, Training Accuracy= 0.95312\n",
      "Iter 51200, Minibatch Loss= 402.918579, Training Accuracy= 0.97656\n",
      "Iter 76800, Minibatch Loss= 419.005615, Training Accuracy= 0.93750\n",
      "Iter 102400, Minibatch Loss= 40.809402, Training Accuracy= 0.99219\n",
      "Iter 128000, Minibatch Loss= 147.570404, Training Accuracy= 0.96094\n",
      "Iter 153600, Minibatch Loss= 86.997498, Training Accuracy= 0.98438\n",
      "Iter 179200, Minibatch Loss= 67.413269, Training Accuracy= 0.98438\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.980469\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print (\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12800, Minibatch Loss= 0.556385, Training Accuracy= 0.81250\n",
      "Iter 25600, Minibatch Loss= 0.350295, Training Accuracy= 0.89062\n",
      "Iter 38400, Minibatch Loss= 0.122831, Training Accuracy= 0.93750\n",
      "Iter 51200, Minibatch Loss= 0.109686, Training Accuracy= 0.95312\n",
      "Iter 64000, Minibatch Loss= 0.085053, Training Accuracy= 0.96875\n",
      "Iter 76800, Minibatch Loss= 0.128308, Training Accuracy= 0.96875\n",
      "Iter 89600, Minibatch Loss= 0.058191, Training Accuracy= 0.96875\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.984375\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print (\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用卷积神经网络做文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pickle import dump,load\n",
    "df = load(open('data/tmdf.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>txt</th>\n",
       "      <th>seg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>本报记者陈雪频实习记者唐翔发自上海\\r\\n　　一家刚刚成立两年的网络支付公司，它的目标是...</td>\n",
       "      <td>本报记者 陈雪频 实习 记者 唐翔 发自 上海 \\r\\n 　 　 一家 刚刚 成立 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>证券通：百联股份未来5年有能力保持高速增长\\r\\n\\r\\n    深度报告 权威内参...</td>\n",
       "      <td>证券 通 ： 百联 股份 未来 5 年 有 能力 保持高速 增长 \\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "      <td>5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "      <td>5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "      <td>5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                txt  \\\n",
       "0      0  　　本报记者陈雪频实习记者唐翔发自上海\\r\\n　　一家刚刚成立两年的网络支付公司，它的目标是...   \n",
       "1      0      证券通：百联股份未来5年有能力保持高速增长\\r\\n\\r\\n    深度报告 权威内参...   \n",
       "2      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....   \n",
       "3      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....   \n",
       "4      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....   \n",
       "\n",
       "                                            seg_word  \n",
       "0  　 　 本报记者 陈雪频 实习 记者 唐翔 发自 上海 \\r\\n 　 　 一家 刚刚 成立 ...  \n",
       "1          证券 通 ： 百联 股份 未来 5 年 有 能力 保持高速 增长 \\r\\n ...  \n",
       "2          5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...  \n",
       "3          5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...  \n",
       "4          5 月 09 日 消息 快评 \\r\\n \\r\\n         深度 报告...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  文本整理完毕，后面建模需要将词汇转成数字编号，可以人工转，也可以让keras转\n",
    "textraw = df.seg_word.values.tolist()\n",
    "#textraw = [line.encode('utf-8') for line in textraw] # 需要存为str才能被keras使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxfeatures = 50000 # 只选择最重要的词\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "token = Tokenizer(nb_words=maxfeatures)\n",
    "token.fit_on_texts(textraw) #如果文本较大可以使用文本流\n",
    "text_seq = token.texts_to_sequences(textraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 2220,\n",
       " 1667,\n",
       " 115,\n",
       " 10317,\n",
       " 219,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 594,\n",
       " 1252,\n",
       " 977,\n",
       " 1028,\n",
       " 3,\n",
       " 278,\n",
       " 1138,\n",
       " 42,\n",
       " 2,\n",
       " 128]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_seq[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median([len(x) for x in text_seq]) #  每条新闻平均400个词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "y = df.label.values # 定义好标签\n",
    "nb_classes = len(np.unique(y))\n",
    "print(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.recurrent  import SimpleRNN, GRU, LSTM\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 600 # 定义文本最大长度\n",
    "batch_size = 32 # 批次\n",
    "word_dim = 100 # 词向量维度\n",
    "nb_filter = 200  # 卷积核个数\n",
    "filter_length = 10 # 卷积窗口大小\n",
    "hidden_dims = 50  # 隐藏层神经元个数\n",
    "nb_epoch = 10      # 训练迭代次数\n",
    "pool_length = 50   # 池化窗口大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(text_seq, y , train_size=0.8, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (14328, 600)\n",
      "X_test shape: (3582, 600)\n"
     ]
    }
   ],
   "source": [
    "# 转为等长矩阵，长度为maxlen\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(train_X, maxlen=maxlen,padding='post', truncating='post')\n",
    "X_test = sequence.pad_sequences(test_X, maxlen=maxlen,padding='post', truncating='post')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将y的格式展开成one-hot\n",
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(train_y, nb_classes)\n",
    "Y_test = np_utils.to_categorical(test_y, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for version bug\n",
    "# import tensorflow as tf\n",
    "# tf.python.control_flow_ops = tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# CNN 模型\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# 词向量嵌入层，输入：词典大小，词向量大小，文本长度\n",
    "model.add(Embedding(maxfeatures, word_dim,input_length=maxlen,dropout=0.25)) \n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode=\"valid\",\n",
    "                        activation=\"relu\"))\n",
    "# 池化层\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "model.add(Flatten())\n",
    "# 全连接层\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12895 samples, validate on 1433 samples\n",
      "Epoch 1/10\n",
      "12895/12895 [==============================] - 363s - loss: 1.4134 - acc: 0.4862 - val_loss: 0.5848 - val_acc: 0.8458\n",
      "Epoch 2/10\n",
      "12895/12895 [==============================] - 354s - loss: 0.5626 - acc: 0.8337 - val_loss: 0.4480 - val_acc: 0.8786\n",
      "Epoch 3/10\n",
      "12895/12895 [==============================] - 342s - loss: 0.3893 - acc: 0.8929 - val_loss: 0.4566 - val_acc: 0.8807\n",
      "Epoch 4/10\n",
      "12895/12895 [==============================] - 343s - loss: 0.2977 - acc: 0.9177 - val_loss: 0.4308 - val_acc: 0.9009\n",
      "Epoch 5/10\n",
      "12895/12895 [==============================] - 355s - loss: 0.2274 - acc: 0.9376 - val_loss: 0.4455 - val_acc: 0.8974\n",
      "Epoch 6/10\n",
      "12895/12895 [==============================] - 363s - loss: 0.1864 - acc: 0.9532 - val_loss: 0.4731 - val_acc: 0.9030\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', patience=1, verbose=1)\n",
    "result = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, \n",
    "            validation_split=0.1, callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582/3582 [==============================] - 27s    \n",
      "Test score: [0.44992601410561206, 0.892518146453391]\n",
      "3582/3582 [==============================] - 27s    \n",
      "Test accuracy: 0.892518146287\n"
     ]
    }
   ],
   "source": [
    "score = earlystop.model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "classes = earlystop.model.predict_classes(X_test, batch_size=batch_size)\n",
    "acc = np_utils.accuracy(classes, test_y) # 要用没有转换前的y\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 练习：用lstm完成文本分类的任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf3]",
   "language": "python",
   "name": "conda-env-tf3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
